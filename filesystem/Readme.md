-	1. Узнайте о sparse (разряженных) файлах.
-	
-	Ответ
-	Хорошее решение , для использование в Torrent, VM-иинфраструктуре.
_____________________________________________________________________________
-	2. Могут ли файлы, являющиеся жесткой ссылкой на один объект, иметь разные права доступа и владельца? Почему?
-	
-	Ответ
-	pi@pi:~/cisco$ touch junipet
-	pi@pi:~/cisco$ ln junipet junipet_link
-	pi@pi:~/cisco$ ls -ilh
-	total 0
-	1055287 -rw-rw-r-- 2 pi pi 0 июн 16 21:06 junipet
-	1055287 -rw-rw-r-- 2 pi pi 0 июн 16 21:06 junipet_link
-	pi@pi:~/cisco$ chmod 0755 junipet
-	pi@pi:~/cisco$ ls -ilh
-	total 0
-	1055287 -rwxr-xr-x 2 pi pi 0 июн 16 21:06 junipet
-	1055287 -rwxr-xr-x 2 pi pi 0 июн 16 21:06 junipet_link$
________________________________________________________________________________________________________
-	3. Сделайте vagrant destroy на имеющийся инстанс Ubuntu. Замените содержимое Vagrantfile следующим:
-	
-	Vagrant.configure("2") do |config|
-	  config.vm.box = "bento/ubuntu-20.04"
-	  config.vm.provider :virtualbox do |vb|
-	    lvm_experiments_disk0_path = "/tmp/lvm_experiments_disk0.vmdk"
-	    lvm_experiments_disk1_path = "/tmp/lvm_experiments_disk1.vmdk"
-	    vb.customize ['createmedium', '--filename', lvm_experiments_disk0_path, '--size', 2560]
-	    vb.customize ['createmedium', '--filename', lvm_experiments_disk1_path, '--size', 2560]
-	    vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk0_path]
-	    vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 2, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk1_path]
-	  end
-	end
-	
-	Ответ
-	vagrant@vagrant:~$ lsblk 
-	NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
-	loop0                       7:0    0 61.9M  1 loop /snap/core20/1328
-	loop1                       7:1    0 43.6M  1 loop /snap/snapd/14978
-	loop2                       7:2    0 67.2M  1 loop /snap/lxd/21835
-	sda                         8:0    0   64G  0 disk 
-	├─sda1                      8:1    0    1M  0 part 
-	├─sda2                      8:2    0  1.5G  0 part /boot
-	└─sda3                      8:3    0 62.5G  0 part 
-	  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm  /
-	sdb                         8:16   0  2.5G  0 disk 
-	sdc                         8:32   0  2.5G  0 disk 
-	vagrant@vagrant:~$ 
_________________________________________________________________________________________________________________
-	4. Используя fdisk, разбейте первый диск на 2 раздела: 2 Гб, оставшееся пространство.
-	
-	Ответ
-	Device     Boot   Start     End Sectors  Size Id Type
-	/dev/sdb1          2048 4104191 4102144    2G 83 Linux
-	/dev/sdb2       4104192 5242879 1138688  556M 83 Linux
____________________________________________________________________________________________________________________
-	5. Используя sfdisk, перенесите данную таблицу разделов на второй диск.
-	
-	Ответ
-	root@vagrant:~# sfdisk -d /dev/sdb | sfdisk /dev/sdc 
-		Checking that no-one is using this disk right now ... OK
-
-	Disk /dev/sdc: 2.51 GiB, 2684354560 bytes, 5242880 sectors
-	Disk model: VBOX HARDDISK   
-	Units: sectors of 1 * 512 = 512 bytes
-	Sector size (logical/physical): 512 bytes / 512 bytes
-	I/O size (minimum/optimal): 512 bytes / 512 bytes
-	
-	>>> Script header accepted.
-	>>> Script header accepted.
-	>>> Script header accepted.
-	>>> Script header accepted.
-	>>> Created a new DOS disklabel with disk identifier 0x75b04b23.
-	/dev/sdc1: Created a new partition 1 of type 'Linux' and of size 2 GiB.
-	/dev/sdc2: Created a new partition 2 of type 'Linux' and of size 556 MiB.
-	/dev/sdc3: Done.
-
-	New situation:
-	Disklabel type: dos
-	Disk identifier: 0x75b04b23
-
-	Device     Boot   Start     End Sectors  Size Id Type
-	/dev/sdc1          2048 4104191 4102144    2G 83 Linux
-	/dev/sdc2       4104192 5242879 1138688  556M 83 Linux
-	
-	The partition table has been altered.
-	Calling ioctl() to re-read partition table.
-	Syncing disks.
_________________________________________________________________________________________________________________
-	6. Соберите mdadm RAID1 на паре разделов 2 Гб.
-	
-	Ответ
-	mdadm --create --verbose /dev/md1 -l 1 -n 2 /dev/sd{b1,c1}
-	mdadm: Note: this array has metadata at the start and
-	    may not be suitable as a boot device.  If you plan to
-	    store '/boot' on this device please ensure that
-	    your boot-loader understands md/v1.x metadata, or use
-	    --metadata=0.90
-	mdadm: size set to 2049024K
-	Continue creating array? y
-	mdadm: Defaulting to version 1.2 metadata
-	mdadm: array /dev/md1 started.
____________________________________________________________________________________________________________________
-	7. Соберите mdadm RAID0 на второй паре маленьких разделов.
-	
-	Ответ
-	mdadm --create --verbose /dev/md0 -l 1 -n 2 /dev/sd{b2,c2}
-	mdadm: Note: this array has metadata at the start and
-	    may not be suitable as a boot device.  If you plan to
-	    store '/boot' on this device please ensure that
-	    your boot-loader understands md/v1.x metadata, or use
-	    --metadata=0.90
-	mdadm: size set to 568320K
-	Continue creating array? y
-	mdadm: Defaulting to version 1.2 metadata
-	mdadm: array /dev/md0 started.
______________________________________________________________________________________________________
-	8. Создайте 2 независимых PV на получившихся md-устройствах.
-	
-	Ответ
-	root@vagrant:~# pvcreate /dev/md1 /dev/md0
-	  Physical volume "/dev/md1" successfully created.
-	  Physical volume "/dev/md0" successfully created.
_________________________________________________________________________________________________________
-	9. Создайте общую volume-group на этих двух PV.
-	
-	Ответ
-	root@vagrant:~# vgcreate vg1 /dev/md1 /dev/md0
-	  Volume group "vg1" successfully created
-	root@vagrant:~# vgdisplay 
-	  --- Volume group ---
-	  VG Name               ubuntu-vg
-	  System ID             
-	  Format                lvm2
-	  Metadata Areas        1
-	  Metadata Sequence No  2
-	  VG Access             read/write
-	  VG Status             resizable
-	  MAX LV                0
-	  Cur LV                1
-	  Open LV               1
-	  Max PV                0
-	  Cur PV                1
-	  Act PV                1
-	  VG Size               <62.50 GiB
-	  PE Size               4.00 MiB
-	  Total PE              15999
-	  Alloc PE / Size       7999 / <31.25 GiB
-	  Free  PE / Size       8000 / 31.25 GiB
-	  VG UUID               4HbbNB-kISH-fXeQ-qzbV-XeNd-At34-cCUUuJ
-	   
-	  --- Volume group ---
-	  VG Name               vg1
-	  System ID             
-	  Format                lvm2
-	  Metadata Areas        2
-	  Metadata Sequence No  1
-	  VG Access             read/write
-	  VG Status             resizable
-	  MAX LV                0
-	  Cur LV                0
-	  Open LV               0
-	  Max PV                0
-	  Cur PV                2
-	  Act PV                2
-	  VG Size               2.49 GiB
-	  PE Size               4.00 MiB
-	  Total PE              638
-	  Alloc PE / Size       0 / 0   
-	  Free  PE / Size       638 / 2.49 GiB
-	  VG UUID               eqCaCZ-lny5-cvnb-jPJj-S8eR-y0BL-feVOGU
__________________________________________________________________________________________________________
-	10. Создайте LV размером 100 Мб, указав его расположение на PV с RAID0.
-	
-	Ответ
-	root@vagrant:~# lvcreate -L 100M vg1 /dev/md0
-	  Logical volume "lvol0" created.
-	root@vagrant:~# vgs
-	  VG        #PV #LV #SN Attr   VSize   VFree 
-	  ubuntu-vg   1   1   0 wz--n- <62.50g 31.25g
-	  vg1         2   1   0 wz--n-   2.49g  2.39g			
-	root@vagrant:~# lvs
-	  LV        VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
-	  ubuntu-lv ubuntu-vg -wi-ao---- <31.25g                                                    
-	  lvol0     vg1       -wi-a----- 100.00m
_______________________________________________________________________________________________________
-	11. Создайте mkfs.ext4 ФС на получившемся LV.
-	
-	Ответ
-	root@vagrant:~# mkfs.ext4 /dev/vg1/lvol0
-	mke2fs 1.45.5 (07-Jan-2020)
-	Creating filesystem with 25600 4k blocks and 25600 inodes
-
-	Allocating group tables: done                            
-	Writing inode tables: done                            
-	Creating journal (1024 blocks): done
-	Writing superblocks and filesystem accounting information: done
________________________________________________________________________________________________________
-	12. Смонтируйте этот раздел в любую директорию, например, /tmp/new.
-	
-	Ответ
-	root@vagrant:~# mkdir /tmp/new
-	root@vagrant:~# mount /dev/vg1/lvol0 /tmp/new/
_________________________________________________________________________________________________________
-	13. Поместите туда тестовый файл, например wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/new/test.gz.
-	
-	Ответ
-	root@vagrant:~# wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/new/test.gz
-	--2022-06-16 19:48:51--  https://mirror.yandex.ru/ubuntu/ls-lR.gz
-	Resolving mirror.yandex.ru (mirror.yandex.ru)... 213.180.204.183, 2a02:6b8::183
-	Connecting to mirror.yandex.ru (mirror.yandex.ru)|213.180.204.183|:443... connected.
-	HTTP request sent, awaiting response... 200 OK
-	Length: 23664590 (23M) [application/octet-stream]
-	Saving to: ‘/tmp/new/test.gz’
-
-	/tmp/new/test.gz                       100%[==========================================================================>]  22.57M  1.88MB/s    in 16s     
-
-	2022-06-16 19:49:08 (1.41 MB/s) - ‘/tmp/new/test.gz’ saved [23664590/23664590]
-
-	root@vagrant:~# ls -la /tmp/new/
-	total 23136
-	drwxr-xr-x  3 root root     4096 Jun 16 19:48 .
-	drwxrwxrwt 12 root root     4096 Jun 16 19:46 ..
-	drwx------  2 root root    16384 Jun 16 19:45 lost+found
-	-rw-r--r--  1 root root 23664590 Jun 16 18:18 test.gz
__________________________________________________________________________________________________________________________
-	14. Прикрепите вывод lsblk.
-	
-	Ответ
-	root@vagrant:~# lsblk
-	NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
-	loop0                       7:0    0 61.9M  1 loop  /snap/core20/1328
-	loop1                       7:1    0 43.6M  1 loop  /snap/snapd/14978
-	loop2                       7:2    0 67.2M  1 loop  /snap/lxd/21835
-	loop3                       7:3    0 61.9M  1 loop  /snap/core20/1518
-	loop4                       7:4    0   47M  1 loop  /snap/snapd/16010
-	loop5                       7:5    0 67.8M  1 loop  /snap/lxd/22753
-	sda                         8:0    0   64G  0 disk  
-	├─sda1                      8:1    0    1M  0 part  
-	├─sda2                      8:2    0  1.5G  0 part  /boot
-	└─sda3                      8:3    0 62.5G  0 part  
-	  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm   /
-	sdb                         8:16   0  2.5G  0 disk  
-	├─sdb1                      8:17   0    2G  0 part  
-	│ └─md1                     9:1    0    2G  0 raid1 
-	└─sdb2                      8:18   0  556M  0 part  
-	  └─md0                     9:0    0  555M  0 raid1 
-	    └─vg1-lvol0           253:1    0  100M  0 lvm   /tmp/new
-	sdc                         8:32   0  2.5G  0 disk  
-	├─sdc1                      8:33   0    2G  0 part  
-	│ └─md1                     9:1    0    2G  0 raid1 
-	└─sdc2                      8:34   0  556M  0 part  
-	  └─md0                     9:0    0  555M  0 raid1 
-	    └─vg1-lvol0           253:1    0  100M  0 lvm   /tmp/new
____________________________________________________________________________________________________________________________________
-	15. Протестируйте целостность файла:
-	-	
-	Ответ
-	root@vagrant:~# gzip -t /tmp/new/test.gz && echo $?
-	0
____________________________________________________________________________________________________________________________________
-	16. Используя pvmove, переместите содержимое PV с RAID0 на RAID1.
-	
-	Ответ
-	root@vagrant:~# pvmove /dev/md0 /dev/md1
-	  /dev/md0: Moved: 100.00%
-	root@vagrant:~# lsblk 
-	NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
-	loop0                       7:0    0 61.9M  1 loop  /snap/core20/1328
-	loop1                       7:1    0 43.6M  1 loop  /snap/snapd/14978
-	loop2                       7:2    0 67.2M  1 loop  /snap/lxd/21835
-	loop3                       7:3    0 61.9M  1 loop  /snap/core20/1518
-	loop4                       7:4    0   47M  1 loop  /snap/snapd/16010
-	loop5                       7:5    0 67.8M  1 loop  /snap/lxd/22753
-	sda                         8:0    0   64G  0 disk  
-	├─sda1                      8:1    0    1M  0 part  
-	├─sda2                      8:2    0  1.5G  0 part  /boot
-	└─sda3                      8:3    0 62.5G  0 part  
-	  └─ubuntu--vg-ubuntu--lv 253:0    0 31.3G  0 lvm   /
-	sdb                         8:16   0  2.5G  0 disk  
-	├─sdb1                      8:17   0    2G  0 part  
-	│ └─md1                     9:1    0    2G  0 raid1 
-	│   └─vg1-lvol0           253:1    0  100M  0 lvm   /tmp/new
-	└─sdb2                      8:18   0  556M  0 part  
-	  └─md0                     9:0    0  555M  0 raid1 
-	sdc                         8:32   0  2.5G  0 disk  
-	├─sdc1                      8:33   0    2G  0 part  
-	│ └─md1                     9:1    0    2G  0 raid1 
-	│   └─vg1-lvol0           253:1    0  100M  0 lvm   /tmp/new
-	└─sdc2                      8:34   0  556M  0 part  
-	  └─md0                     9:0    0  555M  0 raid1 
_________________________________________________________________________________________________________________________________________
-	17. Сделайте --fail на устройство в вашем RAID1 md.
-	
-	Ответ
-	root@vagrant:~# mdadm /dev/md1 --fail /dev/sdb1
-	mdadm: set /dev/sdb1 faulty in /dev/md1
-	root@vagrant:~# mdadm -D /dev/md1
-	/dev/md1:
-	           Version : 1.2
-	     Creation Time : Thu Jun 16 19:26:12 2022
-	        Raid Level : raid1
-	        Array Size : 2049024 (2001.00 MiB 2098.20 MB)
-	     Used Dev Size : 2049024 (2001.00 MiB 2098.20 MB)
-	      Raid Devices : 2
-	     Total Devices : 2
-	       Persistence : Superblock is persistent
-	
-	       Update Time : Thu Jun 16 20:00:33 2022
-             State : clean, degraded 
-	    Active Devices : 1
-	   Working Devices : 1
-	    Failed Devices : 1
-	     Spare Devices : 0
-
-	Consistency Policy : resync
-
-	              Name : vagrant:1  (local to host vagrant)
-	              UUID : addefea8:e3d00982:4d95c774:a6b5640c
-	            Events : 19
-
-	    Number   Major   Minor   RaidDevice State
-	       -       0        0        0      removed
-	       1       8       33        1      active sync   /dev/sdc1
-	
-	       0       8       17        -      faulty   /dev/sdb1
_______________________________________________________________________________________________________________________________________
-	18. Подтвердите выводом dmesg, что RAID1 работает в деградированном состоянии.
-	
-	Ответ
-	root@vagrant:~# dmesg | grep md1
-	[ 2543.974412] md/raid1:md1: not clean -- starting background reconstruction
-	[ 2543.974414] md/raid1:md1: active with 2 out of 2 mirrors
-	[ 2543.974431] md1: detected capacity change from 0 to 2098200576
-	[ 2543.980331] md: resync of RAID array md1
-	[ 2554.196640] md: md1: resync done.
-	[ 4604.153142] md/raid1:md1: Disk failure on sdb1, disabling device.
-	               md/raid1:md1: Operation continuing on 1 devices.
______________________________________________________________________________________________________________________________________
-	19. Протестируйте целостность файла, несмотря на "сбойный" диск он должен продолжать быть доступен:
-	
-	Ответ
-	root@vagrant:~# gzip -t /tmp/new/test.gz
-	root@vagrant:~# gzip -t /tmp/new/test.gz
-	root@vagrant:~# echo $?
-	0
-	root@vagrant:~# gzip -t /tmp/new/test.gz && echo $?
-	0
_________________________________________________________________________________________________________________________________
-	20. Погасите тестовый хост, vagrant destroy.
-	
-	Ответ
-	vagrant destroy -f
-	pi@pi:~/vagrant$ vagrant destroy -f 
-	==> default: Forcing shutdown of VM...
-	==> default: Destroying VM and associated drives... 
